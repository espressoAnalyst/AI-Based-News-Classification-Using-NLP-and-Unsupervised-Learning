{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Xzbdz7TU6M27m8rFWOuv_mDVlSCEP7oZ","timestamp":1763471992863}],"gpuType":"T4","mount_file_id":"1uiejBXea24XhUE2LMBlXjmei6wlRV_-O","authorship_tag":"ABX9TyOFkXMLaFQtuA0znooiENwu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","<center><font size=10>Introduction to LLMs and GenAI</center></font>\n","<center><font size=6>Mini Project 3 - AI-Based News Classification Using NLP and Unsupervised Learning</center></font>"],"metadata":{"id":"uVOpTDcrILun"}},{"cell_type":"markdown","source":["###Business Context\n","\n","\n"],"metadata":{"id":"0K-9MAIXdhCD"}},{"cell_type":"markdown","source":["In the dynamic landscape of the media and news industry, the ability to swiftly categorize and curate content has become a strategic imperative. The vast volume of information demands efficient systems to organize and present content to the audience.\n","\n","The media industry, being the pulse of information dissemination, grapples with the continuous influx of news articles spanning diverse topics. Ensuring that the right articles reach the right audience promptly is not just a logistical necessity but a critical component in retaining and engaging audiences in an age of information overload.\n","\n","Common Industry Challenges: Amidst the ceaseless flow of news, organizations encounter challenges such as:\n","\n","Information Overload: The sheer volume of news articles makes manual categorization impractical.\n","Timeliness: Delays in categorizing news articles can result in outdated or misplaced content."],"metadata":{"id":"X8jrjJYPdrJp"}},{"cell_type":"markdown","source":["###Problem Definition"],"metadata":{"id":"6Brc8V0pdsYY"}},{"cell_type":"markdown","source":["E-news Express, a news aggregation startup, faces the challenge of categorizing the news articles collected. With news articles covering sports, entertainment, politics, and more, the need for an advanced and automated system to categorize them has become increasingly evident. The manual efforts required for categorizing such a diverse range of news articles are substantial, and human errors in the categorization of news articles can lead to reputational damage for the startup. There is also the factor of delays and potential inaccuracies. To streamline and optimize this process, the organization recognizes the imperative of adopting cutting-edge technologies, particularly machine learning, to automate and enhance the categorization of content.\n","\n","As a data scientist on the E-news Express data team, the task is to analyze the text in news articles and build an unsupervised learning model for categorizing them. The categorization done by the model can then be validated against human-defined labels to check the overall accuracy of the AI system. The goal is to optimize the categorization process, ensuring timely and personalized delivery.*"],"metadata":{"id":"G1vjegnydvc0"}},{"cell_type":"code","source":["# installing the sentence-transformers library\n","!pip install -U sentence-transformers -q\n"],"metadata":{"id":"F9ItQcjjD8dt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# to read and manipulate the data\n","import pandas as pd\n","import numpy as np\n","pd.set_option('max_colwidth', None)    # setting column to the maximum column width as per the data\n","\n","# to visualise data\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# to compute distances\n","from scipy.spatial.distance import cdist, pdist\n","from sklearn.metrics import silhouette_score\n","\n","# importing the PyTorch Deep Learning library\n","import torch\n","\n","# to import the model\n","from sentence_transformers import SentenceTransformer\n","\n","# to cluster the data\n","from sklearn.cluster import KMeans\n","\n","# to compute metrics\n","from sklearn.metrics import classification_report\n","\n","# to avoid displaying unnecessary warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n"],"metadata":{"id":"9TXHMea7eAwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W_fhHJzoWh3F"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/Intro to LLM and Gen AI/Mini Project 3/news_articles.csv')"]},{"cell_type":"code","source":["df.head(5)"],"metadata":{"collapsed":true,"id":"aGX3-CQgexad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"Io_aUnc3CD7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"axiGNz6HCHJ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.isnull().sum()"],"metadata":{"id":"kEdlRNdECKTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.duplicated().sum()"],"metadata":{"id":"mEEBVtVwCOeW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.drop_duplicates()"],"metadata":{"id":"4wxOT-sbEOHK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resetting the dataframe index\n","df.reset_index(drop=True, inplace=True)\n","\n","df.duplicated().sum()\n"],"metadata":{"id":"YpBoJpsQErji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.loc[1, 'Text']\n"],"metadata":{"id":"qFq2RtnOEpYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"SItlU9-lFWyd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install hf_xet"],"metadata":{"id":"6wQhnjufGwB3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"],"metadata":{"id":"dWkGLyDqHBND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting the device to GPU if available, else CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# encoding the dataset\n","embedding_matrix = model.encode(df['Text'], show_progress_bar=True, device=device)\n","\n","embedding_matrix.shape\n"],"metadata":{"id":"5PuVoojMJ3Mw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining a function to compute the cosine similarity between two embedding vectors\n","def cosine_score(text1,text2):\n","    # encoding the text\n","    embeddings1 = model.encode(text1)\n","    embeddings2 = model.encode(text2)\n","\n","    # calculating the L2 norm of the embedding vector\n","    norm1 = np.linalg.norm(embeddings1)\n","    norm2 = np.linalg.norm(embeddings2)\n","\n","    # computing the cosine similarity\n","    cosine_similarity_score = ((np.dot(embeddings1,embeddings2))/(norm1*norm2))\n","\n","    return cosine_similarity_score\n"],"metadata":{"id":"KsZCkQhAFF81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a= \"i love apple\"\n","b= \"apple is a fruit\"\n","c= \"i like this table\"\n","print(cosine_score(a,b))\n","print(cosine_score(b,c))\n","print(cosine_score(a,c))\n"],"metadata":{"id":"H_vB5WBvFJZp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# We can also use prebuilt method to calculate similarity score\n","\n","a= \"i love apple\"\n","b= \"apple is a fruit\"\n","c= \"i like this table\"\n","\n","from sentence_transformers import util\n","\n","embeddings1 = model.encode(a)\n","embeddings2 = model.encode(b)\n","embeddings3 = model.encode(c)\n","\n","print(util.cos_sim(embeddings1, embeddings2))\n","print(util.cos_sim(embeddings2, embeddings3))\n","print(util.cos_sim(embeddings1, embeddings3))\n"],"metadata":{"id":"c9PHFHgbFMww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining a function to find the top k similar sentences for a given query\n","def top_k_similar_sentences(embedding_matrix, query_text, k):\n","    # encoding the query text\n","    query_embedding = model.encode(query_text)\n","\n","    # calculating the cosine similarity between the query vector and all other encoded vectors of our dataset\n","    score_vector = np.dot(embedding_matrix,query_embedding)\n","\n","    # sorting the scores in descending order and choosing the first k\n","    top_k_indices = np.argsort(score_vector)[::-1][:k]\n","\n","    # returning the corresponding reviews\n","    return df.loc[list(top_k_indices), 'Text']\n"],"metadata":{"id":"6Cd48QBxFPzL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining the query text\n","query_text = \"Budget for elections\"\n","\n","# displaying the top 5 similar sentences\n","top_k_reviews = top_k_similar_sentences(embedding_matrix, query_text, 5)\n","\n","for i in top_k_reviews:\n","    print(i, end=\"\\n\")\n","    print(\"*******************************************************************\")\n","    print(\"\\n\")\n"],"metadata":{"collapsed":true,"id":"0cF-1yhRFdpS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining the query text\n","query_text = \"High imports and exports\"\n","\n","# displaying the top 5 similar sentences\n","top_k_reviews = top_k_similar_sentences(embedding_matrix, query_text, 5)\n","\n","for i in top_k_reviews:\n","    print(i, end=\"\\n\")\n","    print(\"*******************************************************************\")\n","    print(\"\\n\")\n"],"metadata":{"collapsed":true,"id":"GngfvgRWFkJG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meanDistortions = []\n","clusters = range(2, 11)\n","\n","for k in clusters:\n","    clusterer = KMeans(n_clusters=k, random_state=1)\n","    clusterer.fit(embedding_matrix)\n","\n","    prediction = clusterer.predict(embedding_matrix)\n","\n","    distortion = sum(\n","        np.min(cdist(embedding_matrix, clusterer.cluster_centers_, \"euclidean\"), axis=1) ** 2\n","    )\n","    meanDistortions.append(distortion)\n","\n","    print(\"Number of Clusters:\", k, \"\\tAverage Distortion:\", distortion)\n"],"metadata":{"id":"VVqST21Oj922"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(clusters, meanDistortions, \"bx-\")\n","plt.xlabel(\"k\")\n","plt.ylabel(\"Average Distortion\")\n","plt.title(\"Selecting k with the Elbow Method\", fontsize=20)\n","plt.show()\n"],"metadata":{"id":"P4HPE3M-FrII"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sil_score = []\n","cluster_list = range(2, 11)\n","\n","for n_clusters in cluster_list:\n","    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n","\n","    preds = clusterer.fit_predict((embedding_matrix))\n","\n","    score = silhouette_score(embedding_matrix, preds)\n","    sil_score.append(score)\n","\n","    print(\"For n_clusters = {}, the silhouette score is {})\".format(n_clusters, score))\n"],"metadata":{"id":"cG1Wy9ZmFwME"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(cluster_list, sil_score, \"bx-\")\n","plt.show()\n"],"metadata":{"id":"iki5-N7QFzcm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining the number of clusters/categories\n","n_categories = 5\n","\n","# fitting the model\n","kmeans=KMeans(n_clusters=n_categories,random_state=1).fit(embedding_matrix)"],"metadata":{"id":"J33tC3-CF7Dg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating a copy of the data\n","clustered_data = df.copy()\n","\n","# assigning the cluster/category labels\n","clustered_data['Category'] = kmeans.labels_\n","\n","clustered_data.head()"],"metadata":{"id":"u95NQ6SdF-ms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for each cluster, printing the 5 random news articles\n","for i in range(5):\n","    print(\"CLUSTER\",i)\n","    print(clustered_data.loc[clustered_data.Category == i, 'Text'].sample(5, random_state=1).values)\n","    print(\"*****************************************************************\")\n","    print(\"\\n\")\n"],"metadata":{"id":"YIMjPelQGJLi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dictionary of cluster label to category\n","category_dict = {\n","    0: 'Sports',\n","    1: 'Politics',\n","    2: 'Entertainment',\n","    3: 'Business',\n","    4: 'Technology'\n","}\n","# mapping cluster labels to categories\n","clustered_data['Category'] = clustered_data['Category'].map(category_dict)\n","\n","clustered_data.head()\n"],"metadata":{"id":"zfN0Dt2DGMTE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading the actual labels\n","labels = pd.read_csv(\"/content/drive/MyDrive/Intro to LLM and Gen AI/Mini Project 3/news_article_labels.csv\")\n","# checking the unique labels\n","labels['Label'].unique()\n"],"metadata":{"id":"cTcJL503GWug"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels.head()"],"metadata":{"id":"1P-4PJBOKq61"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels.shape"],"metadata":{"id":"KVj2dpFJK7qz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels.value_counts('Label')"],"metadata":{"id":"h62kqWgFLBmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# adding the actual categories to our dataframe\n","clustered_data['Actual Category'] = labels['Label'].values\n"],"metadata":{"id":"SD3zPvzaGf5d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(classification_report(clustered_data['Actual Category'], clustered_data['Category']))"],"metadata":{"id":"B6RrJRjtGiRD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# creating a dataframe of incorrect categorizations\n","incorrect_category_data = clustered_data[clustered_data['Actual Category'] != clustered_data['Category']].copy()\n","incorrect_category_data.shape\n","\n"],"metadata":{"id":"v1I1Noh8sdmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["incorrect_category_data.head()"],"metadata":{"id":"3Y73FbQAshti"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 24\n","\n","print('Distance from Actual Category')\n","print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[2]], \"euclidean\")[0,0])\n","\n","print('Distance from Predicted Category')\n","print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[3]], \"euclidean\")[0,0])\n"],"metadata":{"id":"Tx9p4gDmsil8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["idx = 45\n","\n","print('Distance from Actual Category')\n","print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[2]], \"euclidean\")[0,0])\n","\n","print('Distance from Predicted Category')\n","print(cdist(embedding_matrix[idx].reshape(1,-1), kmeans.cluster_centers_[[4]], \"euclidean\")[0,0])\n"],"metadata":{"id":"fQddehFVslFg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Conclusion"],"metadata":{"id":"kcaUPk4Xl5ZF"}},{"cell_type":"markdown","source":["##Project Goal\n","\n","1. Built an unsupervised machine learning system to automatically categorize news articles for E-News Express.\n","\n","2. The system helps solve problems like information overload, manual classification errors, and delays in organizing articles.\n","\n","##Technologies & Libraries Used\n","\n","1. Programming was done in Python.\n","\n","2. Used essential data science libraries:\n","\n","- Pandas and NumPy for data loading, cleaning, and preprocessing\n","\n","- Matplotlib and Seaborn for visualization\n","\n","3. Used Sentence Transformers (all-MiniLM-L6-v2) to convert text into dense numerical embeddings.\n","\n","##Data Preprocessing\n","\n","1. Loaded the dataset and removed duplicates.\n","\n","2. Cleaned the data and prepared the text for embedding.\n","\n","3. Embedding & Similarity\n","\n","4. Generated dense sentence embeddings for each article using Sentence Transformers.\n","\n","5. Performed cosine similarity searches to find the most relevant articles for any query.\n","\n","##Unsupervised Clustering\n","\n","1. Applied K-Means Clustering to group articles based on semantic similarity.\n","\n","2. Determined the optimal number of clusters using:\n","\n","3. Elbow Method (distortion scores)\n","\n","4. Silhouette Score\n","\n","6. Selected 5 clusters as the most appropriate.\n","\n","7. Mapping Clusters to Real Categories\n","\n","8. Analyzed each cluster and labeled them according to common themes like:\n","\n","- Sports\n","\n","- Politics\n","\n","- Entertainment\n","\n","- Business\n","\n","- Technology\n","\n","##Model Evaluation\n","\n","1. Compared the cluster labels with human-labeled categories from an external file.\n","\n","2. Generated a classification report to measure:\n","\n","- Accuracy\n","\n","- Precision\n","\n","- Recall\n","\n","- F1-score\n","\n","##Error & Distance Analysis\n","\n","1. Identified articles that were categorized incorrectly.\n","\n","2. Analyzed the distance between article embeddings and cluster centers to understand misclassifications.\n","\n","3. Found cases where articles were semantically close to multiple clusters.\n","\n","##Final Outcome\n","\n","- The project demonstrates how modern NLP + unsupervised learning can automate news categorization.\n","\n","- It reduces manual work, improves personalization, and supports large-scale news management for media organizations."],"metadata":{"id":"0mybDhYMl81y"}}]}